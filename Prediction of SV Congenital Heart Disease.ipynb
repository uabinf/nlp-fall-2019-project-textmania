{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT CODE FOR PREDICTING SV STATUS\n",
    "\n",
    "\n",
    "\n",
    "# Load libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7533c01119e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#NLTK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#Pandas, yay\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter, ExcelFile\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#SkLearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from xgboost import plot_importance\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "\n",
    "#Other\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "#Word Cloud Libraries\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Read in the data\n",
    "df_txpl = pd.read_excel('txpl_project_updated.xlsx')\n",
    "\n",
    "#Shape of the data\n",
    "df_txpl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cute Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't want NA in our cute word cloud...it will make it ugly\n",
    "data = df_txpl.dropna(subset=['CHD_OTHSP','SPECOTH','SURGERY_HISTORY'])\n",
    "\n",
    "#Conat all the strings we want\n",
    "text = str(data.CHD_OTHSP) + str(data.SPECOTH) + str(data.SPECOTH)\n",
    "\n",
    "#Make it a heart shape because PHTS\n",
    "heart_mask = np.array(Image.open(\"heart2.jpg\"))\n",
    "                    \n",
    "wordcloud = WordCloud(mask=heart_mask,background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\",)\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure we have the columns and it imported \n",
    "print(df_txpl.columns)\n",
    "\n",
    "#The categorical variables needed to one hot encode\n",
    "cat_var = [key for key in dict(df_txpl.dtypes) if dict(df_txpl.dtypes)[key] in ['object']] \n",
    "\n",
    "#Do not want to hot encode these they are our text fields of interest.  \n",
    "cat_var.remove('CHD_OTHSP')\n",
    "cat_var.remove('SPECOTH')\n",
    "cat_var.remove('SURGERY_HISTORY')\n",
    "\n",
    "#show the categorical\n",
    "print(cat_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This unnamed column gets added as an index from importing with pandas, not sure how to drop it in the \n",
    "#import so just dropping it here\n",
    "df_txpl.drop(df_txpl.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "\n",
    "#Looking at the data specs\n",
    "print(df_txpl.shape)\n",
    "print(df_txpl.head())\n",
    "print(df_txpl.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode everything categorical\n",
    "df_processed = pd.get_dummies(df_txpl, prefix_sep=\"_\",columns=cat_var)\n",
    "\n",
    "#Text variables to drop from the first model\n",
    "text = [key for key in dict(df_processed.dtypes) if dict(df_processed.dtypes)[key] in ['object']] \n",
    "\n",
    "#These are the text variables now because we transformed the others\n",
    "print(text)\n",
    "\n",
    "#Look at how many people are flagged as Heterotaxy to ensure it was the amount Tobias thought\n",
    "print(df_processed['SV_GROUP'].value_counts())\n",
    "\n",
    "#Set to only SV just to ensure I am getting 1803\n",
    "df_filtered_list = df_processed[df_processed['SV_GROUP'] == 1]\n",
    "\n",
    "#I'm just filtering above to see different numbers for testing\n",
    "print(df_processed.shape)\n",
    "print(df_filtered_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model without text fields\n",
    "df_model1 = df_processed.drop(text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model1\n",
    "\n",
    "#Look at missing\n",
    "def missing(dff):\n",
    "    print (round((dff.isnull().sum() * 100/ len(dff)),2).sort_values(ascending=False))\n",
    "\n",
    "#Eliminate missing data that is more than 25%\n",
    "filter_missing = df_model1[df_model1.columns[df_model1.isnull().mean() < 0.25]]\n",
    "\n",
    "missing(filter_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute with the median\n",
    "df_imp = filter_missing.fillna(filter_missing.median())\n",
    "\n",
    "df_imp\n",
    "\n",
    "#Look at our outcome ~1800 are single ventricle\n",
    "sns.set(font_scale=1.5)\n",
    "countplt=sns.countplot(x='SV_GROUP', data=df_imp, palette ='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data with Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every column except those that woudn't make sense\n",
    "#Patient ID is the ID of the patient, the target variable and then the three surgeries that\n",
    "#Define a patient having single ventricle - if I let CHD_HLH CHD_DILB CHD_TRIAT in it will bias the result\n",
    "cols_all = [col for col in df_imp.columns if col not in ['PATIENT_ID','SV_GROUP','CHD_HLH','CHD_TRIAT','CHD_DILV','CHD_SV']]\n",
    "\n",
    "#Using ALL features in PHTS\n",
    "data_all = df_imp[cols_all]\n",
    "\n",
    "#Only variables with less than 25% missing\n",
    "print(\"Percent Missing for Features Included in Model\")\n",
    "print(missing(filter_missing[cols_all]))\n",
    "\n",
    "#The predictor\n",
    "target = df_imp['SV_GROUP']\n",
    "\n",
    "#Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_all,target, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Features which Give the Most Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Altering this to display mutual information because there are negative values in age so chisq cannot be determined\n",
    "#Because we have negative values in a variable (AGE) because babies can be put on the weight list before they are born\n",
    "#https://stackoverflow.com/questions/43643278/how-do-i-selectkbest-using-mutual-information-from-a-mixture-of-discrete-and-con\n",
    "class SelectKBestCustom(SelectKBest):\n",
    "\n",
    "    # Changed here\n",
    "    def fit(self, X, y, discrete_features='auto'):\n",
    "        X, y = check_X_y(X, y, ['csr', 'csc'], multi_output=True)\n",
    "\n",
    "        if not callable(self.score_func):\n",
    "            raise TypeError(\"The score function should be a callable, %s (%s) \"\n",
    "                        \"was passed.\"\n",
    "                        % (self.score_func, type(self.score_func)))\n",
    "\n",
    "        self._check_params(X, y)\n",
    "\n",
    "        # Changed here also\n",
    "        score_func_ret = self.score_func(X, y, discrete_features)\n",
    "        if isinstance(score_func_ret, (list, tuple)):\n",
    "            self.scores_, self.pvalues_ = score_func_ret\n",
    "            self.pvalues_ = np.asarray(self.pvalues_)\n",
    "        else:\n",
    "            self.scores_ = score_func_ret\n",
    "            self.pvalues_ = None\n",
    "\n",
    "        self.scores_ = np.asarray(self.scores_)\n",
    "        return self\n",
    "\n",
    "#Doing some feature engineering just to see which variables are important in the dataset\n",
    "bestfeatures = SelectKBest(mutual_info_classif, k=10)\n",
    "fit = bestfeatures.fit(data_all,target)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(data_all.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(15,'Score'))  #print 10 best features\n",
    "\n",
    "#This is so interesting, these are the top features usually clinicians say are most predictive of SV CHD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Made this so I can just run this on all my models and get basically any summary statistic I want \n",
    "def modelPrediction(model,data_train,target_train,target_test,data_test,model_name):\n",
    "    pred = model.fit(data_train, target_train).predict(data_test)\n",
    "    Accuracy = accuracy_score(target_test, pred)\n",
    "    f1 = f1_score(target_test, pred, average='weighted')\n",
    "    precision = precision_score(target_test, pred, average=\"weighted\")\n",
    "    recall = recall_score(target_test, pred, average=\"weighted\")   \n",
    "    AUC = roc_auc_score(target_test, pred, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(target_test, pred).ravel()\n",
    "    pd.options.display.float_format = '{:,.3f}'.format\n",
    "    results = pd.DataFrame(\n",
    "    {\n",
    "    'Model Name': [model_name],\n",
    "    'Accuracy': [Accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1 Score': [f1],\n",
    "    'AUC': [AUC],\n",
    "    'True Negative': [tn],\n",
    "    'False Positive': [fp],\n",
    "    'False Negative': [fn],\n",
    "    'True Positive': [tp],\n",
    "    })\n",
    "    return results.stack()\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "print(modelPrediction(dt,X_train,Y_train,Y_test,X_test,\"Decision Tree\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xg = XGBClassifier()\n",
    "print(modelPrediction(xg,X_train,Y_train,Y_test,X_test,\"XG Boost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I think this is neat\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "#plot feature importance\n",
    "plt.style.use('classic')\n",
    "plot_importance(model,max_num_features=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a while to run (like a WHILE), results are similar to logistic regression\n",
    "# svc_model = SVC(kernel='linear')\n",
    "# print(modelPrediction(svc_model,X_train,Y_train,Y_test,X_test,\"SVM\",data,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will get convergence error if we do not increase the interations\n",
    "logit = LogisticRegression(max_iter=300)\n",
    "\n",
    "print(modelPrediction(logit,X_train,Y_train,Y_test,X_test,\"Logisitc Regression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding first text field - SURGERY HISTORY USING TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Only introducing one text field so keeping these out\n",
    "df_model2 = df_processed.drop(['SPECOTH', 'CHD_OTHSP','CHD_SV','PATIENT_ID','CHD_HLH','CHD_TRIAT','CHD_DILV'], axis = 1) \n",
    "\n",
    "#Every column \n",
    "cols_temp2 = [col for col in df_model2.columns]\n",
    "\n",
    "#All but predictor\n",
    "df_model2 = df_model2[cols_temp2]\n",
    "\n",
    "#Remove punctuation\n",
    "df_model2[\"SURG_NO_PUNC\"] = df_model2['SURGERY_HISTORY'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Drop SURGERY_HISTORY because I created SURG_NO_PUNC\n",
    "df_model_drop = df_model2.drop('SURGERY_HISTORY',axis=1)\n",
    "\n",
    "#If there are any missing data fill it with blank\n",
    "df_model_drop['SURG_NO_PUNC'].fillna('', inplace=True)\n",
    "\n",
    "#TDIDF VECTORIZER\n",
    "tfidf = TfidfVectorizer(analyzer='word', stop_words = 'english',tokenizer=word_tokenize)\n",
    "\n",
    "#Transform\n",
    "score = tfidf.fit_transform(df_model_drop['SURG_NO_PUNC'])\n",
    "\n",
    "#Get the column names of the words\n",
    "df = pd.DataFrame(score.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "#Drop surg_no_punc\n",
    "df_model_drop = df_model_drop.drop('SURG_NO_PUNC',axis=1)\n",
    "\n",
    "#Concatenate this back to our original data frame with categorical and continuous features\n",
    "df_fin = pd.concat([df_model_drop.reset_index(drop=True),df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Making sure this didn't mess up anything, till expect 1803\n",
    "print(df_fin['SV_GROUP'].value_counts())\n",
    "\n",
    "#Get rid of missing less than 25 percent\n",
    "filter_missing_fin = df_fin[df_fin.columns[df_fin.isnull().mean() < 0.25]]\n",
    "\n",
    "filter_missing_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute with the median\n",
    "df_imp2 = filter_missing_fin.fillna(df_fin.median())\n",
    "\n",
    "#Eliminate columns with duplicate names - duplicate words\n",
    "df_imp2 = df_imp2.loc[:,~df_imp2.columns.duplicated()]\n",
    "\n",
    "#Select all but predictor\n",
    "first_text = [col for col in df_imp2.columns if col not in ['SV_GROUP']]\n",
    "\n",
    "#If we only want to look at JUST the text, no continuous features\n",
    "only_text = [col for col in df_imp2.columns if col not in df_imp.columns]\n",
    "\n",
    "#The data with all columns but target\n",
    "data_2 = df_imp2[first_text]\n",
    "\n",
    "#The data with all columns but target\n",
    "data_2_ot = df_imp2[only_text]\n",
    "\n",
    "#The predictor\n",
    "target_2 = df_imp2['SV_GROUP']\n",
    "\n",
    "#Split the data\n",
    "X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(data_2,target_2, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train_2.shape)\n",
    "print(X_test_2.shape)\n",
    "print(Y_train_2.shape)\n",
    "print(Y_test_2.shape)\n",
    "\n",
    "#Ensure we have the correct values\n",
    "print(X_train_2.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Features + Surgery History Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All models used:\n",
    "\n",
    "print(modelPrediction(logit,X_train_2,Y_train_2,Y_test_2,X_test_2,\"Logistic Regression\"))\n",
    "\n",
    "#Commenting this one out because it takes a LONG TIME to run. The results are very similar to logisitc regression results\n",
    "# print(modelPrediction(svc_model,X_train_2,Y_train_2,Y_test_2,X_test_2,\"SVM\"))\n",
    "\n",
    "print(modelPrediction(dt,X_train_2,Y_train_2,Y_test_2,X_test_2,\"Decision Tree\"))\n",
    "\n",
    "print(modelPrediction(xg,X_train_2,Y_train_2,Y_test_2,X_test_2,\"XG Boost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD REMAINING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#New covariate list with first text field added\n",
    "covariate_list_txt = ['CHD_OTHSP']\n",
    "\n",
    "#Add these columns\n",
    "cols2 = [col for col in df_processed.columns if col in covariate_list_txt]\n",
    "\n",
    "df_model3 = df_processed[cols2]\n",
    "\n",
    "#Remove punctuation\n",
    "df_model3[\"CHD_NO_PUNC\"] = df_model3['CHD_OTHSP'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Drop CHD_OTHSP because I created CHD_NO_PUNC\n",
    "df_model_drop2 = df_model3.drop('CHD_OTHSP',axis=1)\n",
    "\n",
    "#Changed all blanks to missing\n",
    "df_model_drop2['CHD_NO_PUNC'].fillna('', inplace=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', stop_words = 'english',tokenizer=word_tokenize)\n",
    "score = tfidf.fit_transform(df_model_drop2['CHD_NO_PUNC'])\n",
    "\n",
    "#New data frame containing the tfidf features and their scores\n",
    "df2 = pd.DataFrame(score.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "df_fin2 = pd.concat([df_imp2.reset_index(drop=True),df2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Look at how many people are flagged as Heterotaxy to ensure it was the amount Tobias thought\n",
    "print(df_fin2['SV_GROUP'].value_counts())\n",
    "\n",
    "#Get rid of missing less than 25 percent\n",
    "filter_missing_wt2 = df_fin2[df_fin2.columns[df_fin2.isnull().mean() < 0.25]]\n",
    "\n",
    "filter_missing_wt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp3 = filter_missing_wt2\n",
    "\n",
    "#Eliminate duplicate columns with duplicate words\n",
    "df_imp3 = df_imp3.loc[:,~df_imp3.columns.duplicated()]\n",
    "\n",
    "first_text = [col for col in df_imp3.columns if col not in ['SV_GROUP']]\n",
    "\n",
    "only_text = [col for col in df_imp3.columns if col not in df_imp.columns]\n",
    "\n",
    "#The data with all columns but target\n",
    "data_3 = df_imp3[first_text]\n",
    "\n",
    "#The predictor\n",
    "target_3 = df_imp3['SV_GROUP']\n",
    "\n",
    "#Split the data\n",
    "X_train_3, X_test_3, Y_train_3, Y_test_3 = train_test_split(data_3,target_3, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train_3.shape)\n",
    "print(X_test_3.shape)\n",
    "print(Y_train_3.shape)\n",
    "print(Y_test_3.shape)\n",
    "\n",
    "print(X_train_3.columns.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelPrediction(logit,X_train_3,Y_train_3,Y_test_3,X_test_3,\"Logistic Regression\"))\n",
    "\n",
    "print(modelPrediction(dt,X_train_3,Y_train_3,Y_test_3,X_test_3,\"Decision Tree\"))\n",
    "\n",
    "# print(modelPrediction(svc_model,X_train_3,Y_train_3,Y_test_3,X_test_3,\"SVM\"))\n",
    "\n",
    "print(modelPrediction(xg,X_train_3,Y_train_3,Y_test_3,X_test_3,\"XG Boost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only text fields Surgery History + CHD Specify\n",
    "# Can just text fields predict Single Ventricle Status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data with all columns but target\n",
    "data_3 = df_imp3[only_text]\n",
    "\n",
    "#The predictor\n",
    "target_3 = df_imp3['SV_GROUP']\n",
    "\n",
    "#Split the data\n",
    "X_train_3_text, X_test_3_text, Y_train_3_text, Y_test_3_text = train_test_split(data_3,target_3, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train_3_text.shape)\n",
    "print(X_test_3_text.shape)\n",
    "print(Y_train_3_text.shape)\n",
    "print(Y_test_3_text.shape)\n",
    "\n",
    "print(X_train_3_text.columns.values.tolist())\n",
    "\n",
    "print(modelPrediction(logit,X_train_3_text,Y_train_3_text,Y_test_3_text,X_test_3_text,\"Logistic Regression\"))\n",
    "\n",
    "print(modelPrediction(dt,X_train_3_text,Y_train_3_text,Y_test_3_text,X_test_3_text,\"Decision Tree\"))\n",
    "\n",
    "#Just commenting it out because it takes a long time to run-results were similiar to logistic regrssion\n",
    "#print(modelPrediction(svc_model,X_train_3_text,Y_train_3_text,Y_test_3_text,X_test_3_text,\"SVM\"))\n",
    "\n",
    "print(modelPrediction(xg,X_train_3_text,Y_train_3_text,Y_test_3_text,X_test_3_text,\"XG Boost\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier().fit(X_train_3, Y_train_3)\n",
    "plot_importance(model,max_num_features=5)\n",
    "plt.style.use('classic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio2vec Word Embeddings Instead of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tobias made this using bio2vec\n",
    "df_bio = pd.read_excel('row2vect.xlsx')\n",
    "\n",
    "#Sorting my dataframe by Patient ID to ensure when I concatenate this it will be in the same order\n",
    "df_processed.sort_values(by=['PATIENT_ID'])\n",
    "\n",
    "#Combine with our original dataset, everything is sorted by patient ID \n",
    "df_biovec = pd.concat([df_processed.reset_index(drop=True),df_bio.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We don't want to include these guys because CHD_DILV CHD_TRIAT AND CHD_HLH are surgeries that are ALWAYS single ventricle\n",
    "#I don't want them to bias the model\n",
    "ex_vars = ['CHD_DILV','CHD_TRIAT','CHD_HLH','PATIENT_ID','SURGERY_HISTORY','CHD_SV']\n",
    "\n",
    "#Add these columns\n",
    "cols2 = [col for col in df_biovec.columns if col not in ex_vars]\n",
    "\n",
    "#Set this up for machine learning\n",
    "df_biovec2 = df_biovec[cols2]\n",
    "\n",
    "#Get rid of missing less than 25 percent\n",
    "filter_missing_wt = df_biovec2[df_biovec2.columns[df_biovec2.isnull().mean() < 0.25]]\n",
    "\n",
    "#Impute with the median\n",
    "df_imp4 = filter_missing_wt.fillna(df_fin.median())\n",
    "\n",
    "#All columns but the target\n",
    "all = [col for col in df_imp4.columns if col not in ['SV_GROUP']]\n",
    "\n",
    "#Only word embeddings\n",
    "cols_only_text = [col for col in df_imp4.columns if col not in df_processed.columns]\n",
    "\n",
    "#Data with everything\n",
    "data_4 = df_imp4[all]\n",
    "\n",
    "#Data with only text\n",
    "data_4t = df_imp4[cols_only_text]\n",
    "\n",
    "#The predictor\n",
    "target_4 = df_imp4['SV_GROUP']\n",
    "\n",
    "#Split the data\n",
    "X_train_4, X_test_4, Y_train_4, Y_test_4 = train_test_split(data_4,target_4, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train_4.shape)\n",
    "print(X_test_4.shape)\n",
    "print(Y_train_4.shape)\n",
    "print(Y_test_4.shape)\n",
    "\n",
    "print(X_train_4.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(modelPrediction(xg,X_train_4,Y_train_4,Y_test_4,X_test_4,\"XG Boost\"))\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train_4, Y_train_4)\n",
    "plot_importance(model,max_num_features=5)\n",
    "plt.style.use('classic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only word embeddings from bio2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train_4t, X_test_4t, Y_train_4t, Y_test_4t = train_test_split(data_4t,target_4, test_size = 0.2)\n",
    "\n",
    "#Print dimensions\n",
    "print(X_train_4t.shape)\n",
    "print(X_test_4t.shape)\n",
    "print(Y_train_4t.shape)\n",
    "print(Y_test_4t.shape)\n",
    "\n",
    "print(X_train_4t.columns.values.tolist())\n",
    "\n",
    "print(modelPrediction(xg,X_train_4t,Y_train_4t,Y_test_4t,X_test_4t,\"XG Boost\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
